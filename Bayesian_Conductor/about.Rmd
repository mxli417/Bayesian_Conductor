---
output: html_document
---

## Bayesian Conductor - a simple bayesian model

The intuition behind this small app is that ticket inspections during train 
rides can be mathematically understood as events with only two possible 
outcomes: you either experience a ticket inspection or you do not. 

Hence, the underlying distribution for ticket inspections during a 
number of train rides $n$ is the $Bin(n,p)$ - distribution. But how can 
we estimate our probability $p$ of experiencing a ticket inspection 
during our train rides? And, in case that we do not have 
sufficient data to do this, how can we avoid to assume that we
don't get inspected and be caught by a conductor off-guard? 

This is where we can put [bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) to good use. It allows us
to go into our analysis with some (reasonable) assumptions about how
probable ticket inspections are during our train rides. Additionally, 
we can update this belief in the light of data which we can for 
example accumulate by riding trains and taking note wether or not 
we get inspected. 

This is possible because our estimation of the parameter $p$ results 
from the product of the likelihood from the data and our initial 
assumptions about the probability of ticket inspections (the so-called *priors*):

$f(p|x) = \frac{f(x|p) \: f(p)}{\int f(x|p') \: f(p') \: dp'}$


Here, the final result - the estimate of $p$ for the probability of 
ticket inspections - stems from the expectation of the 
resulting posterior density.

As this is a very simple example, the maths behind all this are relatively
benign. Assuming a Beta-distributed prior $Beta(\alpha, \beta)$ and
binomially distributed data $Bin(n,p)$, we are in the convenient situation in which the
prior distribution and the posterior distribution stem from the same
distribution family, and are both, albeit different $Beta(\alpha, \beta)$ 
distributions ([*conjugate distributions*](https://en.wikipedia.org/wiki/Conjugate_prior)).

Here we see the abstract derivation of the posterior in our situation, 
assuming a $Beta(\alpha, \beta)$ - prior:



$$ 
\begin{aligned}
\ f(\theta|x) = \frac{f(x|\theta) f(\theta)}{\int f(x|p') \: f(p') \: dp'} \\
\ \\
\ \propto \: \: f(x|\theta) \: f(\theta) \\
\ \propto \: \:   \prod_{i\:=1}^{n} {n\choose x_i} \theta^{x_i} (1-\theta)^{1-x_i} \: \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \theta^{\: \alpha-1}(1-\theta)^{\: \beta - 1} \\
\ \propto \: \: \theta^{\: \sum_{i=1}^{n}x_i}(1-\theta)^{\: n-\sum_{i=1}^{n}x_i}\theta^{\: \alpha-1}(1-\theta)^{\: \beta-1}\\
\ \propto \: \: \theta^{\: \sum_{i=1}^{n}x_i + \alpha -1}(1-\theta)^{n-\sum_{i=1}^{n}x_i+\beta-1}\\
\ \\
\ \rightarrow \theta|x \sim Beta(\alpha^{*}, \beta^{*})\\
\ \\
\ with: \\
\ \\
\ \alpha^{*} = \: \sum_{i=1}^{n}x_i + \alpha \\
\ \beta^{*}  = n-\sum_{i=1}^{n}x_i+\beta
\end{aligned}
$$

Using these results, we can estimate our probability $p$ to get inspected
during a number of train rides $n$ as:

$$ \mathbb{E} \: [\theta | x] = \frac{\alpha^{*}}{\alpha^{*}+\beta^{*}}$$
Which simply is the expectation of the $Beta(\alpha^{*}, \beta^{*})$ distribution,
which we have cast over our main parameter of interest, $p$.

In the app, the user can enter individual data entries into the data table
and subsequently display the empirical risk of ticket inspections for a variety
of train types as well as the estimated risk of ticket inspection, using our
prior beliefs about $p$ and the data. 

The estimation becomes more stable with more data, as the app uses all entries
containing information about ticket inspections pertaining to a certain train type.
The individual bayesian estimates of the risk of a ticket inspection are of 
course also calculated and stored in the main data table.

## Prioris

We might reasonably assume that the risk of getting into a ticket inspection
differs between different sorts of trains. In an ICE, as fast-moving city-to-city
express, it is very certain that an individual have his ticket inspected, and
it is more or less mandatory to also have a ticket if you book an individual seat
on such a train. In a local commuter train (S-Bahn), on the other hand it might
not always be the case that a ticket inspection officer is riding your train. 
Hence, the assumed risk to experience a ticket inspection has to be lower. 
Lastly, regional or local trains, which cover greater distances than commuter trains
might be somewhere in between. 

Hence the prioris used here are:

- for ICEs: $Beta(\alpha=15, \beta=2)$

- for RE/RBs: $Beta(\alpha = 8, \beta = 4$

- for the S-Bahn: $Beta(\alpha = 4, \beta = 4)$

```{r echo=FALSE, eval=TRUE}
library(stats)

### plot different types of Beta dists
p = seq(0,1, length=100)
plot(p, dbeta(p, 15, 2), ylab="density", type ="l", col=4)
lines(p, dbeta(p, 8, 4), type ="l", col=3)
lines(p, dbeta(p, 4, 4), type ="l", col=2)
legend(0.4, 5,c("Be(15,2) - ICE","Be(8,4) - RE/RB","Be(4,4) - SBahn"),lty=c(1,1),col=c(4,3))
```

As we can easily see, the different priors are peaked quite differently and
hence carry quite distinct assumptions about our main parameter of interest,
$p$.

## Limitations

Of course, knowing the risk of ticket inspections before entering a train
would be advantageous - but this is not really possible with this app. 

In order to come up with a more realistic estimate for this risk, the app
has to be fed some data (as is always the case with bayesian inference). The 
prior assumptions (listed above) might be helpful, but they can also be wildly
speculative (as is probably the case here).

Hence, this app doesn't lend itself to fare-dodging, which is of course a
criminal offence which only can be discouraged by the author (please pay
your dues!).

The next and maybe even more serious limitation stems from the problem 
of correctly sampling information from train rides. An individual riding
his or her commuter trains might miss some ticket inspections merely by chance
or have a heavily inspected commute day in, day out. From this experience
one cannot readily infer the overall probability of inspection for all train
customers. 

To correctly infer the overall ticket inspection probability for all customers,
one would have to sample them correctly, get the sampled users **all** to
report their experiences and then aggregate the data. Hence, this app 
is merely a small exercise in bayesian inference based on an interesting
example - I hope you enjoy playing around with it and have some fun!

If you want to report bugs & other issues, feel free to use: [Issue tracker](https://github.com/mxli417/Bayesian_Conductor/issues)

